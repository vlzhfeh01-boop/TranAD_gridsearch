/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
torch.Size([387594, 128, 11])
<class 'dict'>
<class 'dict'>
[92mLoading pre-trained model: TranAD[0m
Traceback (most recent call last):
  File "/home/junki/TranAD_gridsearch/main.py", line 144, in <module>
    model, optimizer, scheduler, epoch, accuracy_list = load_model(
  File "/home/junki/TranAD_gridsearch/src2/utils.py", line 200, in load_model
    model.load_state_dict(checkpoint["model_state_dict"])
  File "/home/junki/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1667, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for TranAD:
	size mismatch for pos_encoder.pe: copying a param with shape torch.Size([20, 1, 8]) from checkpoint, the shape in current model is torch.Size([20, 1, 11]).
	size mismatch for transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([24, 8]) from checkpoint, the shape in current model is torch.Size([33, 11]).
	size mismatch for transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([33]).
	size mismatch for transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([11, 11]).
	size mismatch for transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([11]).
	size mismatch for transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([8, 11]).
	size mismatch for transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([11, 8]).
	size mismatch for transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([11]).
	size mismatch for transformer_decoder1.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([24, 8]) from checkpoint, the shape in current model is torch.Size([33, 11]).
	size mismatch for transformer_decoder1.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([33]).
	size mismatch for transformer_decoder1.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([11, 11]).
	size mismatch for transformer_decoder1.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([11]).
	size mismatch for transformer_decoder1.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([24, 8]) from checkpoint, the shape in current model is torch.Size([33, 11]).
	size mismatch for transformer_decoder1.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([33]).
	size mismatch for transformer_decoder1.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([11, 11]).
	size mismatch for transformer_decoder1.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([11]).
	size mismatch for transformer_decoder1.layers.0.linear1.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([8, 11]).
	size mismatch for transformer_decoder1.layers.0.linear2.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([11, 8]).
	size mismatch for transformer_decoder1.layers.0.linear2.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([11]).
	size mismatch for transformer_decoder2.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([24, 8]) from checkpoint, the shape in current model is torch.Size([33, 11]).
	size mismatch for transformer_decoder2.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([33]).
	size mismatch for transformer_decoder2.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([11, 11]).
	size mismatch for transformer_decoder2.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([11]).
	size mismatch for transformer_decoder2.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([24, 8]) from checkpoint, the shape in current model is torch.Size([33, 11]).
	size mismatch for transformer_decoder2.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([33]).
	size mismatch for transformer_decoder2.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([11, 11]).
	size mismatch for transformer_decoder2.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([11]).
	size mismatch for transformer_decoder2.layers.0.linear1.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([8, 11]).
	size mismatch for transformer_decoder2.layers.0.linear2.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([11, 8]).
	size mismatch for transformer_decoder2.layers.0.linear2.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([11]).
	size mismatch for fcn.0.weight: copying a param with shape torch.Size([8, 8]) from checkpoint, the shape in current model is torch.Size([11, 11]).
	size mismatch for fcn.0.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([11]).
	size mismatch for input_proj.weight: copying a param with shape torch.Size([8, 16]) from checkpoint, the shape in current model is torch.Size([11, 22]).
	size mismatch for input_proj.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([11]).
